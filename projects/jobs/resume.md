---
###  Data Analyst w/advanced SQL Skills
Wired4Health, Inc

https://docs.google.com/document/d/1GwFDLlgn1rnStQr3gCBxGJYXlYetOJ0A6X0KnyL8mwI/edit#

Interview

1  Data Analysis / Data Integration
2  Indexes / Triggers / Functions / Procedures
3  Database Design
4  RDBMS basics
5  Unix Linux
6  ETL tools
5  Past Projects
	- NDD
	- Exact match
	- Reporting
6  Oracle
7  MySql and HBase
8  Java Python Scala
	- oops principles
9  Visulaisaion for data loading
10 Install Oracle and python data use visualisation

---


Resume:
---
Over 6+ years of experience in designing and developing software products and performing data analytics
Over 3 years of experience in handling huge data sets using  Sql, Nosql and Hadoop ecosystem
Designed and Developed ETL pipelines for data ingestion and calculating Analytics in tight SLA’s 
Adept in Advanced SQL like Indexes and Triggers and Database design
Experienced in writing Oracle UDF’s and PLSQL for custom functionality
Experienced in optimizing Oracle sql/plsql scripts for meeting tight SLA’s
NOSQL databases. Previously used ORACLE, MYSQL and HBASE. Knowledgeable in cassandra
Experience in Spark core, Spark SQL and spark streaming using SCALA and Python API.
Researcher and developer in predictive modeling and big data analytics using Python, Spark API
Developed projects using OOPs and functional programming using JAVA and SCALA
Knowledgeable in Advanced Data modeling using python SCIKIT learn and visualisation graphs using Python MatplotLib and R
Advanced Interactive Visualization using D3js and Python
Solid background in Core Java concepts like Threads, Collections Framework, Java Reflection. And also have hands-on experience in building Class diagrams, activity diagrams, sequence diagrams, activity diagrams and flowcharts using Visio.
Strong database connectivity skills to RDBMS and NOSQL databases. 
Exploratory Data Analysis for feature engineering and building Data Models and testing.
Knowledgeable in advanced algorithms.
Adept in handling xml, json, avro, parquet, HDF5 formats
Hands on experience in using JUnit, Log4J for debugging and testing the applications.
Has good domain knowledge in Financial, Retail and Insurance domains 
Solid experience in communicating with all levels of enterprise
Ability to work independently, lead a development team and meet project goals successfully.
Experience in working in Agile Environment along with Jira and Confluence tools.
---

PROFESSIONAL EXPERIENCE:
WESTERN UNION, SFO, CA                                                  		 	 May 15 – Nov 16
Big data Engineer
Western union is one of the largest and oldest international money transfer. As a big data engineer I was involved in implementation and design of many big data applications along with the interactive visualizations for risk and fraud teams. I have designed and implemented risk assessment systems and finding blacklist customer identification systems. This was done on Oracle database by pl sql scripts and by using UDF’s.
Worked on developing ingesting data and calculating KPI metrics in  near real time. Also developed complex risk variable calculations using spark and MR, by joining different huge datasets.  Researched and developed predictive models for finding genuine old customers in real time. Developed real-time monitoring systems, which analyses risk by calculating millions of business metrics. Also developed beautiful interactive visualisation using d3.js and Html
Responsibilities:
As a developer I was responsible for end to end life cycle on multiple ETL projects and also helped in solving business problems by designing and implementing POC’s.
Design and development of real time fuzzy lookup for finding old returning customers using Oracle and Java UDF.
Responsible for ETL jobs between Hadoop ecosystem and RDBMS data stores, using SQOOP, SPARK, HBASE, HDFS as Output sink, Oracle, MYSQL, IBM Netezza as input resources
Design and development of Java real-time monitoring systems, which reports possible fraud on billions of business metrics.
Building web monitoring system using Spring Boot for reporting data metrics using Phoenix and HBASE.
Developing SPARK sql  jobs and hive sql  scripts for data transformations. 
Developed SPARK STREAMING jobs for real-time data ingestion from Kafka.
Developed Hive tables and optimized queries to fetch the data from HDFS. 
Worked in AWS for Near Duplicate Detection POC project. Deployed a web service on top of Oracle EC2 Instances for elastic heavy computations. 
Developed Reporting using Python, Flask, PySpark and reports using Jupyter.
Designed and Developed front end UI for customer registrations using D3.js and leaflets for geo interactive maps.
Communicated status, issues, concerns, designs and technical decisions to the business in the daily Scrum as well as using Jira and Confluence.
Performed code review and code documentation also helped in automating Q/A testing.

Environment: Hadoop 2.2, SPARK,HDFS, CDH5, Hbase, Flume, Kafka, Sqoop, Eclipse, Java, Python, Scala, Jira, Confluence, Shell Scripting, Jupyter, AngularJS, D3.js
  
  
  
### COX, Atlanta, GA                                                      		 	 	Jan 14- May 15
Hadoop Developer
Cox Communications represents the third largest cable provider in the nation. As a Hadoop developer I was involved in implementation of a recommendation application for Interactive Television (ITV) viewers. The project is related to e-marketplace public portal and enables buyers of "pay-per-view" basis to perform business transactions online.  This required ingesting the data into HDFS using flume and sqoop, writing map reduce using recommendation algorithms. 
Responsibilities:
As a developer am responsible to handle ETL pipelines from Oracle to HDFS using sqoop scripts
Developed Hive sql scripts and Oracle scripts for daily analytics
Imported data using Flume and Sqoop for ingesting data into Hadoop File System (HDFS)
Designed and developed mysql databases for handling daily user analytics. This database provides data for frontend to show customer metrics for Marketing and sales teams.
Developed scripts and batch jobs to schedule various hadoop jobs.
Developed and maintained Hive databases and developed automated scripts for replicating the Oracle and MySql databases. 
Responsible for ingesting data into HDFS from unstructured user logs and xmls using flume.
Troubleshoot and resolve ingestion issues related to Flume .
Developed Map Reduce jobs to support the recommendation application. Used Mahout Machine learning Library for implementing user based recommendation engine.
Communicated status, issues, concerns, designs and technical decisions to the business in the daily Scrum as well as in Jira.
Participated in code review sessions.
Worked with other team members in writing, reviewing, estimating and testing the code.
Environment: Hadoop 2.0.0, MR1, HDFS, CDH4, Flume, Mahout, Sqoop, Eclipse, Java, Java Regex, Jira, Shell Scripting, Python.

---

job Description

This remote telecommuting full time (45 - 55+ hours per week) opportunity involves data analysis and data integration with hospital and healthcare data (e.g., claims, charges, labs, pharmacy, EMR, etc.) using the client's existing SQL/Oracle/MySQL, Ruby/PHP, Unix/Linux, and existing data interface and ETL tools. You must be able to demonstrate examples of past projects involving data analysis with large data sets where advanced SQL/DB skills were utilized. You must also have basic programming competence with one or more Object Oriented Language(s) - such as Ruby, PHP, Python, C Sharp, Java, etc. If you do not have these skill sets you are not qualified for this opportunity and your resume will not be considered.

Technical Skills

Candidates should have 5+ years experience in the following:
- Very advanced working experience with RDBMS and SQL/ETL. Preferably with Oracle and Mysql. (Note: You may be tested prior to engagement);
- Unix / Linux shell scripting;
- Programming experience preferably with OOP language but if not a good understanding of the principles of object oriented programming; and
- healthcare experience a plus but not a requirement.

Location:


Contractor, Remote, Telecommuting. 

All candidates selected are subject to a one-time criminal background check and one time drug screening test.

Must be eligible to work in the USA (social security number or Corp) without ANY sponsorship. 

Compensation:


Based upon your experience, hours per week and length of engagement, data integration analysts earn between $90K and $110K+ per year based upon average hours worked per week. An average of 45-55+ hours per week from your location is a typical workload but weekly hours are not capped. No travel required. This opportunity is a long term engagement for the right candidates.

Apply Now
Posted By
Heathrow, FL
--- 
