###

SPARK
AWS
SCALA

We are hiring Sr Data Engineers for a large client in the Media

Adtech/Programmatic advertising Space!


Data engineer with Spark and Scala
location : San Francisco
Duration: 6 months
Required Skills
. Spark (1+ years experience)
. Scala (skill level: 5 or more out of 10)
. ETL experience (At least 1 past project)
. Dealt with Multiple Tera bytes of data for ETL using Spark
. Hive
Nice to have skills
. AWS
. Hadoop Map-reduce
. Java
Total experience: 3 to 8 years
Best Regards,
Nalini
Sr. Technical Recruiter
Protean Solutions Inc.
SBA Certified 8(a) SDB
Minority Women Owned Business
510-552 9430 (phone) / 408-608-1654 (fax)
Click here to Apply

---

Professional Summary 
Over 6+ years of experience in designing and developing ETL pipelines and performing data analytics
Over 3 years of experience in building realtime and batch applications using Hadoop Ecosystem (Map/Reduce, Hive, Pig, Flume, Sqoop, Spark, Kafka)
Researcher and developer in predictive modeling and big data analytics using spark  and python
Developed projects using OOPs using JAVA and SCALA
Data analysis and graphs using PYTHON and R
Adept in SQL and NOSQL databases. Previously used ORACLE, MYSQL and HBASE. Knowledgeable in cassandra
Experience with Machine Learning supervised learning algorithms. Used Scikit-learn and Spark ML Library for feature engineering and predictive modelling
Experience in Spark core, Spark SQL and spark streaming using SCALA and Python API.
Interactive Visualization using D3js 
Statistical and data analytics using Python and R
Solid background in Core Java concepts like Threads, Collections Framework, Java Reflection. And also have hands-on experience in building Class diagrams, activity diagrams, sequence diagrams, activity diagrams and flowcharts using Visio.
Significant experience in J2EE technologies including Servlets, JSP, Spring, Struts, EJB, JMS, Hibernate, JDBC, XML, XSLT and JNDI for building various client-server applications and Unix Shell Scripting
Strong database connectivity skills which includes Oracle, MYSQL and in programming with SQL, PL/SQL, and Stored Procedures, Triggers, Functions and 
Experienced in developing web application using Spring BOOT and Spring MVC.
Have good knowledge in Predictive Modeling, Statistical Learning Theory, Support Vector Machines (SVM), Decision Trees, Neural Networks.
Exploratory Data Analysis, for feature engineering and building Data Models and testing.
Adept in handling xml, json, avro, parquet, HDF5 formats
Hands on experience in using JUnit, Log4J for debugging and testing the applications.
Has good domain knowledge in Financial, Retail and Insurance domains 
Has good knowledge in graph databases using Titan and SCALA
Solid experience in communicating with all levels of enterprise
Ability to work independently, lead a development team.
Experience in working in Agile Environment along with Jira and Confluence tools.


PROFESSIONAL EXPERIENCE:
WESTERN UNION, SFO, CA                                                  		 	 May 15 – Nov 16
Hadoop Developer
Western union is one of the largest and oldest international money transfer. As a big data engineer I was involved in implementation and design of applications along with the interactive visualizations for risk and fraud teams. I have designed and implemented risk assessment systems and finding blacklist customer identification systems. 
Worked on developing near real time KPI metrics and risk variable calculations using spark streaming. Researched and developed applications using predictive models for finding genuine old customers in tight SLAs. Developed real-time monitoring systems, which analyses risk by calculating millions of business metrics by using spark. Also developed interactive visualisation using d3.js and Html
Responsibilities:
As a developer I was responsible for end to end life cycle on multiple projects and also helped in solving business problems by designing and implementing POC’s.
Responsible for ETL jobs between Hadoop ecosystem and RDBMS data stores, using SCALA, SQOOP, SPARK, HBASE, HDFS as Output sink, Oracle, MYSQL, IBM Netezza as input resources
Design and development of real-time monitoring systems, which reports possible fraud on billions of business metrics.
Design and development of real time fuzzy lookup for finding old returning customers using Oracle and Java UDF.
Building web monitoring system using Spring Boot for reporting data metrics, used Phoenix on HBASE for batch processing.
Developed SPARK , Hive sql scripts and Hive UDFs for data transformations. 
Developed SPARK STREAMING using SCALA api for real-time data ingestion from Kafka and realtime analytics update.
Experienced in using Python, Flask, PySpark and data driven web reports using Jupyter.
Proficient in functional programming using SCALA  
Worked in AWS for Near Duplicate Detection POC project. Deployed a web service on top of Oracle EC2 Instances. 
Designed and Developed UX UI visualisations for customer registrations using D3.js and leaflets for geo interactive maps.
Communicated status, issues, concerns, designs and technical decisions to the business in the daily Scrum as well as using Jira and Confluence.
Performed code review and code documentation also helped in automating Q/A testing.

Environment: Hadoop 2.2, SPARK, MR2, HDFS, CDH5, Phoenix, Hbase, Flume, Kafka, Sqoop, Eclipse, Java, Python, Scala, Jira, Confluence, Shell Scripting, Jupyter, AngularJS, D3.js
COX, Atlanta, GA                                                      		 	 	Jan 14- May 15
Hadoop Developer
Cox Communications represents the third largest cable provider in the nation. As a Hadoop developer I was involved in implementation of a recommendation application for Interactive Television (ITV) viewers. The project is related to e-marketplace public portal and enables buyers of "pay-per-view" basis to perform business transactions online.  This required ingesting the data into HDFS using flume and sqoop, writing map reduce using recommendation algorithms. 
Responsibilities:
As a developer am responsible to handle ETL pipelines from Oracle to HDFS using sqoop scripts
Developed Hive sql scripts and Oracle scripts for daily analytics
Imported data using Flume and Sqoop for ingesting data into Hadoop File System (HDFS)
Designed and developed mysql databases for handling daily user analytics. This database provides data for frontend to show customer metrics for Marketing and sales teams.
Developed scripts and batch jobs to schedule various hadoop jobs.
Developed and maintained Hive databases and developed automated scripts for replicating the Oracle and MySql databases. 
Responsible for ingesting data into HDFS from unstructured user logs and xmls using flume.
Troubleshoot and resolve ingestion issues related to Flume .
Developed Map Reduce jobs to support the recommendation application. Used Mahout Machine learning Library for implementing user based recommendation engine.
Communicated status, issues, concerns, designs and technical decisions to the business in the daily Scrum as well as in Jira.
Worked with other team members in writing, reviewing, estimating and testing the code. As a developer I am responsible to handle ETL pipelines from Oracle to HDFS using sqoop scripts
Environment: Hadoop 2.0.0, MR1, HDFS, CDH4, Flume, Mahout, Sqoop, Eclipse, Java, Java Regex, Jira, Shell Scripting, Python.

Client: Verizon, Dallas, TX	 					              April 13 - Jan 14
Hadoop/Java Developer
Project Description: Verizon’s employee portal provides ability to search employee browsing habits at work. To increase employee productivity and ensure security of company information, Verizon’s employee portal tracks employee email and internet browsing collects various data like urls, transmitting data size into HDFS. Hadoop ecosystem is used to collect and analyze the data using Map Reduce jobs. Flume servers are installed on the network proxy servers to collect the data and store into HDFS. Collected data is statistically analyzed for anomalies and analised against plugable rules. THis are presented to employee portal built using java Spring MVC, Java Script and . 

Responsibilities:
Involved in functional requirement review. Worked closely with Risk & Compliance Team and BA. 
Designed and configured Flume servers to collect data from the network proxy servers and store to HDFS.
Developed data pipeline using Flume and Java map reduce to ingest employee browsing data into HDFS for analysis.
Used agent E2E Chain for reliability and failover in flume. 
Designed and implemented Map Reduce jobs for analyzing the data collected by the flume server. 
Designed and implemented RESTFul APIs to retrieve the data from Hadoop Platform to Employee Portal Web Application.
Developed MRUnit tests for unit testing the Map Reduce jobs.
Used Apache Log4J for logging. 
Facilitated Knowledge transfer sessions.
Worked in an agile environment.

Technology & Tools:
Hadoop 2.0.0 MR1, CDH4, HDFS, Flume 0.9.3, Sqoop 1.x, Hive, Java 1.6, Spring 3.x, Eclipse Juno, XML, JSON, Maven. 

CVS, Richardson, TX                                                               		                  Aug  12 – Apr 13
Java/J2EE Developer

Project Description:  This project was developed to help the company differentiate an enterprise system based on the client showing different prices for the existing clients. We also changed templates based on the clients. Based on this we added new fields in different JSPs like batch Adjustment Detail, batch Summary, and batch Adjustment Review.
Responsibilities:
Involved in quickly understanding the requirements and reporting the manager about the daily status of this project as this was a very urgent need for the company.
Extensively used   design pattern like Application façade, Data Access object and MVC.
Worked with different layers like POJOs, dao, transactions, service and presentation.
Developed a presentation layer using JSP, CSS and Javascript.
Understood the existing code and involved in helping and explaining the team members about the flow of the code.
Involved in testing the code locally and fixing the bugs reported by the QA.
Involved in debugging the code and replicating the problems reported by the tester.
Developed different UTPs for the team and the tester and also worked-through the project in the live meeting session.

Environment: Websphere 6.1, JAVA, JSP, HTML, JavaScript, CSS, Serena Dimensions, DB2, JavaBeans, Design Pattern, ANT

Ivision Solutions Pvt Ltd                                                               		 May 10 – Aug 12 Java/J2EE Developer
Project Description: Resource Planning, Pricing and Project Management (RESP) is an e-Procurement application for direct and indirect materials procurements through internet and send Quotations, receive Orders, enter delivery data, create Invoices, view payment details and print reports using this online system.
Responsibilities:
Used WebSphere, which has high performance and full-integrated Java platform for Enterprise Applications
Actively involved in component development, deployment for the application interface.
Strongly followed the coding standards and implemented MVC Design Patterns. 
Involved in creating EJBs that handle business logic and persistence of data.
Involved in impact analysis of Change requests and Bug fixes.
Unit testing and integration testing of the modules.
Involved in testing the code locally and fixing the bugs reported by the QA.
Automated testing scripts.
Java Naming/Directory Interface (JNDI) to support transparent access to distributed components.
Environment: Sybase, WebSphere Studio Application Developer WSAD, Enterprise Javabeans (EJB), Struts, WebSphere Application Server, HTML, Java.
Academic Qualification:
Bachelor degree in Information Technology, JNTU, INDIA.

---
	
